{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning hyperparameters with Keras tuner"
      ],
      "metadata": {
        "id": "p8tnN3JRDUrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "mf9A7_P1DYw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # loads a library to mount your google drive\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "print('Mounting drive...')\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/data/\" # shows all files \n",
        "\n",
        "print('Reading data...')\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/data/yaleExtB_data.npy\" # sets the path to the data folder\n",
        "X = np.load(data_path)\n",
        "# data = torch.from_numpy(np.load(data_path))\n",
        "\n",
        "print('Reading target data...')\n",
        "target_path = \"/content/drive/My Drive/Colab Notebooks/data/yaleExtB_target.npy\"\n",
        "target = np.load(target_path)\n",
        "\n",
        "print('One hot encoding...')\n",
        "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
        "\n",
        "reshaped=target.reshape(len(target), 1)\n",
        "\n",
        "Y=onehot_encoder.fit_transform(reshaped)\n",
        "\n",
        "print('Train-test split...')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y, \n",
        "                                                    test_size=0.33, \n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "print('Calculating PCA...')\n",
        "# PCA \n",
        "nof_prin_components = 200  # PARAMETER for optimisation in expereiments\n",
        "pca = PCA(n_components=nof_prin_components, whiten=True).fit(X_train)\n",
        "# applies PCA to the train and test images to calculate the principal components\n",
        "X_train_pca = pca.transform(X_train) \n",
        "X_test_pca = pca.transform(X_test)\n",
        "print('Done preparing data.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tFbWQ5sNwWJ",
        "outputId": "b5bc4363-bb0d-464c-fc60-c5bef085618e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting drive...\n",
            "Mounted at /content/drive\n",
            "Tr0.zip  yaleExtB_data.npy  yaleExtB_target.npy\n",
            "Reading data...\n",
            "Reading target data...\n",
            "One hot encoding...\n",
            "Train-test split...\n",
            "Calculating PCA...\n",
            "Done preparing data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnS4vzQjHgGm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If keras-tuner library is not available, install it\n",
        "print('Installing keras-tuner library...')\n",
        "!pip install -q -U keras-tuner\n",
        "print('Finished installing keras-tuner.')"
      ],
      "metadata": {
        "id": "J6IrVCiAIN-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcafaaea-aca0-4b87-de25-ed3a63181ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "dT6bB-MfIYhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a callback function to build a model"
      ],
      "metadata": {
        "id": "KNVY6LAQDrci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp_tuner):\n",
        "\n",
        "  # Creating a sequential model\n",
        "  model = keras.Sequential()\n",
        "  #model.add(keras.layers.Flatten(input_shape=(4)))\n",
        "\n",
        "  # input layer\n",
        "  # model.add(keras.layers.Flatten())   \n",
        "  model.add(keras.layers.Dense(200, activation='relu'))\n",
        "\n",
        "  # tuner parameter for search\n",
        "  # Let's start hidden layer nodes from 64 then go up to 256 with increments \n",
        "  # of 16\n",
        "  tuner_units = hp_tuner.Int('units', min_value=64, max_value=256, step=16)\n",
        "  model.add(keras.layers.Dense(units=tuner_units, activation='relu'))\n",
        "\n",
        "  # model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "  # output layer\n",
        "  model.add(keras.layers.Dense(30, activation='softmax'))\n",
        "\n",
        "  # let's define range of learning rate\n",
        "  tuner_learning_rate = hp_tuner.Choice('learning_rate', values = [0.1, 0.01, 0.001])\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=tuner_learning_rate)\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "kXsgAbAZHqWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the tuner and perform hypertuning\n",
        "\n",
        "There are four tuners available: RandomSearch, Hyperband, BayesianOptimization, Sklearn"
      ],
      "metadata": {
        "id": "bBdihM2TFZXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder, \n",
        "                     objective='val_accuracy', \n",
        "                     max_epochs = 10)\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciBuG8zjMWal",
        "outputId": "2ddb953b-cd17-42b4-a243-6e9f9b32f061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 16, 'sampling': 'linear'}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.01, 0.001], 'ordered': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a callback to stop training when certain validation loss is reached\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANzCLoB0NXU0",
        "outputId": "82dc9585-563c-4632-ed0f-facd62d5765b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 06s]\n",
            "val_accuracy: 0.049751244485378265\n",
            "\n",
            "Best val_accuracy So Far: 0.7711442708969116\n",
            "Total elapsed time: 00h 02m 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for best parameters\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "dvvBAUA4F5KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters\n",
        "best_parameters = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print(f\"Best no. of hidden nodes: {best_parameters.get('units')}\")\n",
        "print(f\"Best learnig rate: {best_parameters.get('learning_rate')}\")"
      ],
      "metadata": {
        "id": "6HL2q9VXF7Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a mode with optimal hyperparameters then train the model"
      ],
      "metadata": {
        "id": "6fwYrk9fGE9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train the new model for 50 epochs\n",
        "model = tuner.hypermodel.build(best_parameters)\n",
        "history = model.fit(X_train_pca, y_train, epochs=50, validation_split=0.2)"
      ],
      "metadata": {
        "id": "1-OEqdKWATTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the best epoch\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "KeBcgBWVGR-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a final model with the best parameters and best epoch"
      ],
      "metadata": {
        "id": "1VyexdWjGk2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model and train for the best epoch\n",
        "hypermodel = tuner.hypermodel.build(best_parameters)\n",
        "hypermodel.fit(X_train_pca, y_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "id": "0NMIPNU3AtvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the final model with test data then obtain accuracy\n",
        "eval_result = hypermodel.evaluate(X_test_pca, y_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILW8U9CwBBsy",
        "outputId": "3d1027fa-707c-4f9c-a888-829aa33e9676"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.9293\n",
            "[test loss, test accuracy]: [0.2831777334213257, 0.9292929172515869]\n"
          ]
        }
      ]
    }
  ]
}